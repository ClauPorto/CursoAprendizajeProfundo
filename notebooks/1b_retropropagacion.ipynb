{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1b_retropropagacion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/master/notebooks/1b_retropropagacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V83__FrBij1f",
        "colab_type": "text"
      },
      "source": [
        "# Retropropagación de errores\n",
        "\n",
        "En este *notebook* programaremos con NumPy una red neuronal densa y la entrenaremos para aproximar la operación XOR usando del gradiente descedente con el algoritmo de retropropagación de errores. Recordemos que la operación XOR ($\\otimes$) está de la siguiente manera:\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |0|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSlnjW4Oi-FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAUmKI5jNuX",
        "colab_type": "text"
      },
      "source": [
        "Nuestra red neuronal densa está compuesta por una capa de 2 entradas ($x_1$ y $x_2$), una capa oculta con 10 neuronas con función de activación sigmoide y una capa de salida con una sola neurona con función de activación sigmoide. Esta función de activación se define como:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhT3i68jf6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoide(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6SyrPhWBrw",
        "colab_type": "text"
      },
      "source": [
        "La función sigmoide tiene una derivada que está expresada en términos de la misma función, esto es, \n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\sigma (z)}{\\partial z} = \\sigma(z) (1 - \\sigma(z))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJxvxKeAjn24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivada_sigmoide(x):\n",
        "    return np.multiply(sigmoide(x), (1.0 - sigmoide(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WxI8FfLXKHv",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver la operación XOR como una tarea de clasificación binaria a partir de 2 entradas. Por lo tanto, usaremos la función de pérdida de entropía cruzada binaria:\n",
        "\n",
        "$$\n",
        "ECB(\\mathbf{y}, \\mathbf{\\hat{y}})  = -\\sum_{i=1}^N \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDjlmpAQjR3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropia_cruzada_binaria(y, p):\n",
        "    p[p == 0] = np.nextafter(0., 1.)\n",
        "    p[p == 1] = np.nextafter(1., 0.)\n",
        "    return -(np.log(p[y == 1]).sum() + np.log(1 - p[y == 0]).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nMdK-RYWMS",
        "colab_type": "text"
      },
      "source": [
        "Asimismo, calcularemos la exactitud para medir el rendimiento del modelo aprendido por la red neuronal densa:\n",
        "\n",
        "$$\n",
        "exactitud = \\frac{correctos}{total}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wxvZq10jIM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exactitud(y, y_predicha):\n",
        "    return (y == y_predicha).mean() * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p02hAdUFZNLL",
        "colab_type": "text"
      },
      "source": [
        "Ahora, definimos la función que propaga hacia adelante una entrada $\\mathbf{x}^{i}$. Como la red está compuesta de 2 capas densas (1 oculta y 1 de salida), tenemos 2 matrices de pesos con sus correspondientes vectores de sesgos $\\{\\mathbf{W}^{\\{1\\}}, \\mathbf{b}^{\\{1\\}}\\}$ y $\\{\\mathbf{W}^{\\{2\\}}, \\mathbf{b}^{\\{2\\}}\\}$ de la capa oculta y la capa de salida respectivamente. Así, podemos llevar a cabo la propagación hacia adelante en esta red de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\t\\begin{split}\n",
        "\t\t\t\t\\mathbf{a}^{\\{1\\}} & =  \\mathbf{x}^{(i)} \\\\\n",
        "\t\t\t\t\\mathbf{z}^{\\{2\\}} & =  \\mathbf{W}^{\\{1\\}} \\cdot \\mathbf{a}^{\\{1\\}} + \\mathbf{b}^{\\{1\\}}\\\\\n",
        "\t\t\t\t\\mathbf{a}^{\\{2\\}} & =  \\sigma(\\mathbf{z}^{\\{2\\}}) \\\\\n",
        "\t\t\t\t\\mathbf{z}^{\\{3\\}} & =  \\mathbf{W}^{\\{2\\}} \\cdot \\mathbf{a}^{\\{2\\}}  + \\mathbf{b}^{\\{2\\}}\\\\\n",
        "\t\t\t\t\\mathbf{a}^{\\{3\\}} & =  \\sigma(\\mathbf{z}^{\\{3\\}})\\\\\n",
        "\t\t\t\t\\hat{y}^{(i)} & =  \\mathbf{a}^{\\{3\\}}\n",
        "\t\t\t\\end{split}\n",
        "      $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAsEk-zajvpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hacia_adelante(x, W1, b1, W2, b2):\n",
        "  z2 = np.dot(W1.T, x[:, np.newaxis]) + b1\n",
        "  a2 = sigmoide(z2)\n",
        "  z3 = np.dot(W2.T, a2) + b2\n",
        "  y_hat = sigmoide(z3)\n",
        "  \n",
        "  return z2, a2, z3, y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiOT6jqXjzwQ",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, definimos la función para entrenar nuestra red neuronal usando gradiente descendente. Para calcular el gradiente de la función de pérdida respecto a los pesos y sesgos en cada capa empleamos el algoritmo de retropropagación de errores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P7i6eLgkJdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retropropagacion(X, y, alpha = 0.01, n_epocas = 100, n_ocultas = 10):\n",
        "    n_ejemplos = X.shape[0]\n",
        "    n_entradas = X.shape[1]\n",
        "    \n",
        "    # Inicialización de las matrices de pesos W y V\n",
        "    W1 = np.sqrt(1.0 / n_entradas) * np.random.randn(n_entradas, n_ocultas)\n",
        "    b1 = np.zeros((n_ocultas, 1))\n",
        "    \n",
        "    W2 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, 1)\n",
        "    b2 = np.zeros((1, 1))\n",
        "    \n",
        "    perdidas = np.zeros((n_epocas))\n",
        "    exactitudes = np.zeros((n_epocas))\n",
        "    y_predicha = np.zeros((y.shape))\n",
        "    for i in range(n_epocas):\n",
        "        for j in range(n_ejemplos):\n",
        "            z2, a2, z3, y_hat = hacia_adelante(X[j], W1, b1, W2, b2)\n",
        "\n",
        "            # cálculo de gradiente para W2 por retropropagación\n",
        "            delta3 = (y_hat - y[j]) * derivada_sigmoide(z3)\n",
        "            W2 = W2 - alpha * np.outer(a2, delta3)\n",
        "            b2 = b2 - alpha * delta3\n",
        "\n",
        "            # cálculo de gradiente para W1 por retropropagación\n",
        "            delta2 = np.dot(W2, delta3) * derivada_sigmoide(z2)\n",
        "            W1 = W1 - alpha * np.outer(X[j], delta2)\n",
        "            b1 = b1 - alpha * delta2\n",
        "\n",
        "            y_predicha[j] = y_hat\n",
        "            \n",
        "        # calcula error en época\n",
        "        perdidas[i] = entropia_cruzada_binaria(y, y_predicha)\n",
        "        exactitudes[i] = exactitud(y, np.round(y_predicha))\n",
        "        print('Epoch {0}: Error = {1} Exactitud = {2}'.format(i, \n",
        "                                                              perdidas[i], \n",
        "                                                              exactitudes[i]))\n",
        "\n",
        "    return W1, W2, perdidas, exactitudes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nau0HWsrkRxg",
        "colab_type": "text"
      },
      "source": [
        "Para probar nuestra red, generamos los ejemplos correspondientes a la operación XOR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8txXZ34GkUAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ejemplo (XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0, 1, 1, 0]]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLT8avfhkYH7",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, entrenamos nuestra red con estos ejemplos por 200 épocas usando una tasa de aprendizaje $\\alpha = 1.0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijKxVwZ3kbyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fc96d96-2f6a-4fef-a823-497e98220756"
      },
      "source": [
        "np.random.seed(0)\n",
        "W1, W2, perdidas, exactitudes = retropropagacion(X, \n",
        "                                                 y, \n",
        "                                                 alpha = 1.0, \n",
        "                                                 n_epocas = 200,\n",
        "                                                 n_ocultas = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Error = 3.4737521571212318 Exactitud = 25.0\n",
            "Epoch 1: Error = 3.4621893539983217 Exactitud = 25.0\n",
            "Epoch 2: Error = 3.4638591836303827 Exactitud = 50.0\n",
            "Epoch 3: Error = 3.4667990850901718 Exactitud = 50.0\n",
            "Epoch 4: Error = 3.4692765311295695 Exactitud = 50.0\n",
            "Epoch 5: Error = 3.4712999939289766 Exactitud = 50.0\n",
            "Epoch 6: Error = 3.4730576791773418 Exactitud = 50.0\n",
            "Epoch 7: Error = 3.474680551473296 Exactitud = 50.0\n",
            "Epoch 8: Error = 3.4762395683680385 Exactitud = 50.0\n",
            "Epoch 9: Error = 3.477769475088765 Exactitud = 50.0\n",
            "Epoch 10: Error = 3.4792860090889137 Exactitud = 50.0\n",
            "Epoch 11: Error = 3.4807954729134245 Exactitud = 50.0\n",
            "Epoch 12: Error = 3.4822996094619056 Exactitud = 50.0\n",
            "Epoch 13: Error = 3.483797989727295 Exactitud = 50.0\n",
            "Epoch 14: Error = 3.485289159117471 Exactitud = 50.0\n",
            "Epoch 15: Error = 3.48677118131892 Exactitud = 50.0\n",
            "Epoch 16: Error = 3.4882418941046383 Exactitud = 50.0\n",
            "Epoch 17: Error = 3.4896990286323075 Exactitud = 50.0\n",
            "Epoch 18: Error = 3.4911402644339606 Exactitud = 50.0\n",
            "Epoch 19: Error = 3.4925632542361824 Exactitud = 50.0\n",
            "Epoch 20: Error = 3.493965634659588 Exactitud = 50.0\n",
            "Epoch 21: Error = 3.4953450303033025 Exactitud = 50.0\n",
            "Epoch 22: Error = 3.496699054706433 Exactitud = 50.0\n",
            "Epoch 23: Error = 3.4980253098019345 Exactitud = 50.0\n",
            "Epoch 24: Error = 3.4993213846049835 Exactitud = 50.0\n",
            "Epoch 25: Error = 3.500584853474096 Exactitud = 25.0\n",
            "Epoch 26: Error = 3.5018132740977728 Exactitud = 25.0\n",
            "Epoch 27: Error = 3.5030041852753735 Exactitud = 25.0\n",
            "Epoch 28: Error = 3.5041551045237114 Exactitud = 25.0\n",
            "Epoch 29: Error = 3.5052635255251907 Exactitud = 25.0\n",
            "Epoch 30: Error = 3.506326915427653 Exactitud = 25.0\n",
            "Epoch 31: Error = 3.5073427120050202 Exactitud = 25.0\n",
            "Epoch 32: Error = 3.5083083206889745 Exactitud = 25.0\n",
            "Epoch 33: Error = 3.5092211114841056 Exactitud = 25.0\n",
            "Epoch 34: Error = 3.510078415781761 Exactitud = 25.0\n",
            "Epoch 35: Error = 3.5108775230909806 Exactitud = 25.0\n",
            "Epoch 36: Error = 3.511615677708261 Exactitud = 25.0\n",
            "Epoch 37: Error = 3.512290075351479 Exactitud = 25.0\n",
            "Epoch 38: Error = 3.512897859786942 Exactitud = 25.0\n",
            "Epoch 39: Error = 3.5134361194823702 Exactitud = 25.0\n",
            "Epoch 40: Error = 3.51390188432237 Exactitud = 25.0\n",
            "Epoch 41: Error = 3.514292122426847 Exactitud = 25.0\n",
            "Epoch 42: Error = 3.5146037371165555 Exactitud = 25.0\n",
            "Epoch 43: Error = 3.514833564073667 Exactitud = 25.0\n",
            "Epoch 44: Error = 3.514978368748766 Exactitud = 25.0\n",
            "Epoch 45: Error = 3.5150348440689525 Exactitud = 25.0\n",
            "Epoch 46: Error = 3.5149996085047386 Exactitud = 25.0\n",
            "Epoch 47: Error = 3.514869204556015 Exactitud = 25.0\n",
            "Epoch 48: Error = 3.5146400977195467 Exactitud = 25.0\n",
            "Epoch 49: Error = 3.514308676002087 Exactitud = 25.0\n",
            "Epoch 50: Error = 3.513871250044213 Exactitud = 25.0\n",
            "Epoch 51: Error = 3.5133240539203663 Exactitud = 25.0\n",
            "Epoch 52: Error = 3.5126632466801118 Exactitud = 25.0\n",
            "Epoch 53: Error = 3.5118849146944426 Exactitud = 25.0\n",
            "Epoch 54: Error = 3.5109850748687297 Exactitud = 25.0\n",
            "Epoch 55: Error = 3.5099596787808798 Exactitud = 25.0\n",
            "Epoch 56: Error = 3.5088046177990435 Exactitud = 25.0\n",
            "Epoch 57: Error = 3.507515729228073 Exactitud = 25.0\n",
            "Epoch 58: Error = 3.506088803527576 Exactitud = 25.0\n",
            "Epoch 59: Error = 3.5045195926369934 Exactitud = 25.0\n",
            "Epoch 60: Error = 3.5028038194345226 Exactitud = 25.0\n",
            "Epoch 61: Error = 3.5009371883470015 Exactitud = 25.0\n",
            "Epoch 62: Error = 3.498915397116976 Exactitud = 25.0\n",
            "Epoch 63: Error = 3.496734149721267 Exactitud = 25.0\n",
            "Epoch 64: Error = 3.4943891704222536 Exactitud = 25.0\n",
            "Epoch 65: Error = 3.4918762189191748 Exactitud = 25.0\n",
            "Epoch 66: Error = 3.489191106551761 Exactitud = 25.0\n",
            "Epoch 67: Error = 3.4863297134927738 Exactitud = 25.0\n",
            "Epoch 68: Error = 3.48328800684963 Exactitud = 50.0\n",
            "Epoch 69: Error = 3.4800620595782346 Exactitud = 50.0\n",
            "Epoch 70: Error = 3.476648070094744 Exactitud = 50.0\n",
            "Epoch 71: Error = 3.4730423824533814 Exactitud = 50.0\n",
            "Epoch 72: Error = 3.4692415069406954 Exactitud = 50.0\n",
            "Epoch 73: Error = 3.46524214091929 Exactitud = 50.0\n",
            "Epoch 74: Error = 3.4610411897369806 Exactitud = 50.0\n",
            "Epoch 75: Error = 3.4566357875012113 Exactitud = 50.0\n",
            "Epoch 76: Error = 3.452023317503306 Exactitud = 50.0\n",
            "Epoch 77: Error = 3.4472014320634745 Exactitud = 50.0\n",
            "Epoch 78: Error = 3.4421680715554723 Exactitud = 50.0\n",
            "Epoch 79: Error = 3.43692148236012 Exactitud = 50.0\n",
            "Epoch 80: Error = 3.4314602334896205 Exactitud = 50.0\n",
            "Epoch 81: Error = 3.425783231620567 Exactitud = 50.0\n",
            "Epoch 82: Error = 3.41988973427279 Exactitud = 50.0\n",
            "Epoch 83: Error = 3.4137793608743996 Exactitud = 50.0\n",
            "Epoch 84: Error = 3.4074521014608403 Exactitud = 50.0\n",
            "Epoch 85: Error = 3.4009083227678616 Exactitud = 50.0\n",
            "Epoch 86: Error = 3.394148771495265 Exactitud = 50.0\n",
            "Epoch 87: Error = 3.387174574540338 Exactitud = 50.0\n",
            "Epoch 88: Error = 3.3799872360271417 Exactitud = 50.0\n",
            "Epoch 89: Error = 3.37258863099012 Exactitud = 50.0\n",
            "Epoch 90: Error = 3.3649809956077203 Exactitud = 50.0\n",
            "Epoch 91: Error = 3.357166913923498 Exactitud = 50.0\n",
            "Epoch 92: Error = 3.3491493010378535 Exactitud = 50.0\n",
            "Epoch 93: Error = 3.3409313828024674 Exactitud = 50.0\n",
            "Epoch 94: Error = 3.3325166721007093 Exactitud = 50.0\n",
            "Epoch 95: Error = 3.323908941849719 Exactitud = 50.0\n",
            "Epoch 96: Error = 3.3151121949122553 Exactitud = 50.0\n",
            "Epoch 97: Error = 3.3061306311575 Exactitud = 50.0\n",
            "Epoch 98: Error = 3.296968611958425 Exactitud = 50.0\n",
            "Epoch 99: Error = 3.2876306224577094 Exactitud = 50.0\n",
            "Epoch 100: Error = 3.278121231973205 Exactitud = 50.0\n",
            "Epoch 101: Error = 3.26844505294651 Exactitud = 50.0\n",
            "Epoch 102: Error = 3.258606698863189 Exactitud = 50.0\n",
            "Epoch 103: Error = 3.248610741589891 Exactitud = 50.0\n",
            "Epoch 104: Error = 3.2384616685815732 Exactitud = 50.0\n",
            "Epoch 105: Error = 3.228163840410896 Exactitud = 50.0\n",
            "Epoch 106: Error = 3.2177214490618766 Exactitud = 50.0\n",
            "Epoch 107: Error = 3.20713847741141 Exactitud = 50.0\n",
            "Epoch 108: Error = 3.1964186602958717 Exactitud = 50.0\n",
            "Epoch 109: Error = 3.1855654475269004 Exactitud = 50.0\n",
            "Epoch 110: Error = 3.1745819691814194 Exactitud = 50.0\n",
            "Epoch 111: Error = 3.1634710034476163 Exactitud = 50.0\n",
            "Epoch 112: Error = 3.1522349472620546 Exactitud = 50.0\n",
            "Epoch 113: Error = 3.1408757899248454 Exactitud = 50.0\n",
            "Epoch 114: Error = 3.129395089831119 Exactitud = 50.0\n",
            "Epoch 115: Error = 3.117793954409124 Exactitud = 50.0\n",
            "Epoch 116: Error = 3.1060730233091425 Exactitud = 50.0\n",
            "Epoch 117: Error = 3.0942324548438425 Exactitud = 50.0\n",
            "Epoch 118: Error = 3.082271915640349 Exactitud = 50.0\n",
            "Epoch 119: Error = 3.070190573427409 Exactitud = 50.0\n",
            "Epoch 120: Error = 3.057987092847713 Exactitud = 50.0\n",
            "Epoch 121: Error = 3.045659634155522 Exactitud = 50.0\n",
            "Epoch 122: Error = 3.0332058546328495 Exactitud = 50.0\n",
            "Epoch 123: Error = 3.0206229125330237 Exactitud = 50.0\n",
            "Epoch 124: Error = 3.007907473337795 Exactitud = 50.0\n",
            "Epoch 125: Error = 2.995055718092592 Exactitud = 50.0\n",
            "Epoch 126: Error = 2.982063353563217 Exactitud = 50.0\n",
            "Epoch 127: Error = 2.9689256239356308 Exactitud = 50.0\n",
            "Epoch 128: Error = 2.955637323757989 Exactitud = 50.0\n",
            "Epoch 129: Error = 2.942192811800465 Exactitud = 50.0\n",
            "Epoch 130: Error = 2.9285860254837908 Exactitud = 50.0\n",
            "Epoch 131: Error = 2.9148104955024134 Exactitud = 50.0\n",
            "Epoch 132: Error = 2.9008593602436803 Exactitud = 50.0\n",
            "Epoch 133: Error = 2.886725379582435 Exactitud = 50.0\n",
            "Epoch 134: Error = 2.8724009476130137 Exactitud = 50.0\n",
            "Epoch 135: Error = 2.8578781038711103 Exactitud = 50.0\n",
            "Epoch 136: Error = 2.8431485426001357 Exactitud = 50.0\n",
            "Epoch 137: Error = 2.8282036196349445 Exactitud = 50.0\n",
            "Epoch 138: Error = 2.8130343565151446 Exactitud = 50.0\n",
            "Epoch 139: Error = 2.7976314415059496 Exactitud = 50.0\n",
            "Epoch 140: Error = 2.7819852273018353 Exactitud = 50.0\n",
            "Epoch 141: Error = 2.766085725321928 Exactitud = 50.0\n",
            "Epoch 142: Error = 2.749922596680225 Exactitud = 50.0\n",
            "Epoch 143: Error = 2.7334851401305618 Exactitud = 50.0\n",
            "Epoch 144: Error = 2.7167622775465095 Exactitud = 50.0\n",
            "Epoch 145: Error = 2.6997425377978783 Exactitud = 50.0\n",
            "Epoch 146: Error = 2.682414040223185 Exactitud = 50.0\n",
            "Epoch 147: Error = 2.6647644792630127 Exactitud = 50.0\n",
            "Epoch 148: Error = 2.646781112200745 Exactitud = 50.0\n",
            "Epoch 149: Error = 2.628450752339504 Exactitud = 50.0\n",
            "Epoch 150: Error = 2.6097597703092896 Exactitud = 50.0\n",
            "Epoch 151: Error = 2.590694106525615 Exactitud = 50.0\n",
            "Epoch 152: Error = 2.5712392980886865 Exactitud = 50.0\n",
            "Epoch 153: Error = 2.5513805235977562 Exactitud = 50.0\n",
            "Epoch 154: Error = 2.531102669437143 Exactitud = 50.0\n",
            "Epoch 155: Error = 2.5103904210480614 Exactitud = 50.0\n",
            "Epoch 156: Error = 2.4892283825164983 Exactitud = 50.0\n",
            "Epoch 157: Error = 2.4676012274670835 Exactitud = 50.0\n",
            "Epoch 158: Error = 2.4454938837456757 Exactitud = 50.0\n",
            "Epoch 159: Error = 2.4228917536919314 Exactitud = 50.0\n",
            "Epoch 160: Error = 2.3997809709446702 Exactitud = 50.0\n",
            "Epoch 161: Error = 2.3761486936879432 Exactitud = 50.0\n",
            "Epoch 162: Error = 2.351983433039436 Exactitud = 50.0\n",
            "Epoch 163: Error = 2.3272754139138696 Exactitud = 50.0\n",
            "Epoch 164: Error = 2.3020169641763673 Exactitud = 50.0\n",
            "Epoch 165: Error = 2.2762029262539674 Exactitud = 50.0\n",
            "Epoch 166: Error = 2.2498310836250655 Exactitud = 50.0\n",
            "Epoch 167: Error = 2.2229025927948394 Exactitud = 50.0\n",
            "Epoch 168: Error = 2.1954224095422976 Exactitud = 50.0\n",
            "Epoch 169: Error = 2.167399696463074 Exactitud = 50.0\n",
            "Epoch 170: Error = 2.1388481972272437 Exactitud = 50.0\n",
            "Epoch 171: Error = 2.1097865616471063 Exactitud = 50.0\n",
            "Epoch 172: Error = 2.080238604759836 Exactitud = 50.0\n",
            "Epoch 173: Error = 2.050233482855764 Exactitud = 75.0\n",
            "Epoch 174: Error = 2.019805769923037 Exactitud = 75.0\n",
            "Epoch 175: Error = 1.988995419530171 Exactitud = 75.0\n",
            "Epoch 176: Error = 1.957847599893741 Exactitud = 75.0\n",
            "Epoch 177: Error = 1.9264123938718298 Exactitud = 100.0\n",
            "Epoch 178: Error = 1.8947443608627044 Exactitud = 100.0\n",
            "Epoch 179: Error = 1.8629019638960909 Exactitud = 100.0\n",
            "Epoch 180: Error = 1.830946872227379 Exactitud = 100.0\n",
            "Epoch 181: Error = 1.7989431569565688 Exactitud = 100.0\n",
            "Epoch 182: Error = 1.7669564039360564 Exactitud = 100.0\n",
            "Epoch 183: Error = 1.7350527737955215 Exactitud = 100.0\n",
            "Epoch 184: Error = 1.7032980426456608 Exactitud = 100.0\n",
            "Epoch 185: Error = 1.6717566584452732 Exactitud = 100.0\n",
            "Epoch 186: Error = 1.640490846916244 Exactitud = 100.0\n",
            "Epoch 187: Error = 1.6095597973749924 Exactitud = 100.0\n",
            "Epoch 188: Error = 1.5790189533301344 Exactitud = 100.0\n",
            "Epoch 189: Error = 1.548919425820341 Exactitud = 100.0\n",
            "Epoch 190: Error = 1.5193075399953386 Exactitud = 100.0\n",
            "Epoch 191: Error = 1.4902245181227616 Exactitud = 100.0\n",
            "Epoch 192: Error = 1.4617062956526774 Exactitud = 100.0\n",
            "Epoch 193: Error = 1.4337834616103091 Exactitud = 100.0\n",
            "Epoch 194: Error = 1.406481310618243 Exactitud = 100.0\n",
            "Epoch 195: Error = 1.3798199912811668 Exactitud = 100.0\n",
            "Epoch 196: Error = 1.353814734366729 Exactitud = 100.0\n",
            "Epoch 197: Error = 1.3284761439722488 Exactitud = 100.0\n",
            "Epoch 198: Error = 1.3038105354379717 Exactitud = 100.0\n",
            "Epoch 199: Error = 1.2798203049229713 Exactitud = 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8A3KZ5JkDJ3",
        "colab_type": "text"
      },
      "source": [
        "Graficamos el valor de la pérdida y la exactitud en cada época para ver el comportamiento de nuestra red durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yglJSF9nkR7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "059109ba-d686-42e1-b81d-1fa09bf01872"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0OaZtsCCgs",
        "colab_type": "text"
      },
      "source": [
        "## Inicializando los pesos con zeros\n",
        "Como se mencionó anteriormente, las matrices de pesos $\\mathbf{W^{\\{1\\}}}$ y $\\mathbf{W^{\\{2\\}}}$ se initializan con valores aleatorios pequeños mientras que los vectores de sesgo $\\mathbf{b^{\\{1\\}}}$ y $\\mathbf{b^{\\{1\\}}}$ con zeros. Examinemos qué pasa si inicializamos las matrices de pesos con zeros. Observa los valores de los pesos en cada época."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHaavbWGzqzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retropropagacion_zeros(X, y, alpha = 0.1, n_epocas = 100, n_ocultas = 10):\n",
        "    n_ejemplos = X.shape[0]\n",
        "    n_entradas = X.shape[1]\n",
        "    \n",
        "    # Inicializa matrices de pesos W1 y W2 y vectores de sesgos b1 y b2\n",
        "    W1 = np.zeros((n_entradas, n_ocultas))\n",
        "    b1 = np.zeros((n_ocultas, 1)) \n",
        "    W2 = np.zeros((n_ocultas, 1))\n",
        "    b2 = np.zeros((1, 1))\n",
        "    \n",
        "    perdidas = np.zeros((n_epocas))\n",
        "    exactitudes = np.zeros((n_epocas))\n",
        "    y_predicha = np.zeros((y.shape))\n",
        "    for i in range(n_epocas):\n",
        "        for j in range(n_ejemplos):\n",
        "            z2, a2, z3, y_hat = hacia_adelante(X[j], W1, b1, W2, b2)\n",
        "\n",
        "            # cálculo de gradiente para W2 por retropropagación\n",
        "            delta3 = (y[j] - y_hat) * derivada_sigmoide(z3)\n",
        "            W2 = W2 - alpha * np.outer(a2, delta3)\n",
        "            b2 = b2 - alpha * delta3\n",
        "            \n",
        "            # calculo de gradiente para W1 por retropropagación\n",
        "            delta2 = np.dot(W2, delta3) * derivada_sigmoide(z2)\n",
        "            W1 = W1 - alpha * np.outer(X[j], delta2)\n",
        "            b1 = b1 - alpha * delta2\n",
        "            \n",
        "            y_predicha[j] = y_hat\n",
        "            \n",
        "        # calcula error en época\n",
        "        perdidas[i] = entropia_cruzada_binaria(y, y_predicha)\n",
        "        exactitudes[i] = exactitud(y, np.round(y_predicha))\n",
        "        print('Epoch {0}: Error = {1} Exactitud = {2}'.format(i, \n",
        "                                                              perdidas[i], \n",
        "                                                              exactitudes[i]))\n",
        "        print('W1 = {0}'.format(W1))\n",
        "        print('W2 = {0}'.format(W2))\n",
        "            \n",
        "    return W1, W2, perdidas, exactitudes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr1HownICHf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df1437c2-8cac-41af-f4a1-1e6a4bf4522e"
      },
      "source": [
        "W1, W2, perdidas, exactitudes = retropropagacion_zeros(X, \n",
        "                                                       y, \n",
        "                                                       alpha = 1.0,\n",
        "                                                       n_epocas = 5,\n",
        "                                                       n_ocultas = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Error = 2.383938269247035 Exactitud = 100.0\n",
            "W1 = [[ 0.0015006   0.0015006   0.0015006   0.0015006   0.0015006   0.0015006\n",
            "   0.0015006   0.0015006   0.0015006   0.0015006 ]\n",
            " [-0.00013937 -0.00013937 -0.00013937 -0.00013937 -0.00013937 -0.00013937\n",
            "  -0.00013937 -0.00013937 -0.00013937 -0.00013937]]\n",
            "W2 = [[0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]]\n",
            "Epoch 1: Error = 2.3940150790588524 Exactitud = 75.0\n",
            "W1 = [[3.07843345e-03 3.07843345e-03 3.07843345e-03 3.07843345e-03\n",
            "  3.07843345e-03 3.07843345e-03 3.07843345e-03 3.07843345e-03\n",
            "  3.07843345e-03 3.07843345e-03]\n",
            " [6.32326402e-05 6.32326402e-05 6.32326402e-05 6.32326402e-05\n",
            "  6.32326402e-05 6.32326402e-05 6.32326402e-05 6.32326402e-05\n",
            "  6.32326402e-05 6.32326402e-05]]\n",
            "W2 = [[0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]]\n",
            "Epoch 2: Error = 2.454375736775318 Exactitud = 50.0\n",
            "W1 = [[0.00535668 0.00535668 0.00535668 0.00535668 0.00535668 0.00535668\n",
            "  0.00535668 0.00535668 0.00535668 0.00535668]\n",
            " [0.00153339 0.00153339 0.00153339 0.00153339 0.00153339 0.00153339\n",
            "  0.00153339 0.00153339 0.00153339 0.00153339]]\n",
            "W2 = [[0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]]\n",
            "Epoch 3: Error = 2.7471355126181987 Exactitud = 50.0\n",
            "W1 = [[0.01036404 0.01036404 0.01036404 0.01036404 0.01036404 0.01036404\n",
            "  0.01036404 0.01036404 0.01036404 0.01036404]\n",
            " [0.00645381 0.00645381 0.00645381 0.00645381 0.00645381 0.00645381\n",
            "  0.00645381 0.00645381 0.00645381 0.00645381]]\n",
            "W2 = [[0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]]\n",
            "Epoch 4: Error = 3.5902678548185696 Exactitud = 50.0\n",
            "W1 = [[0.01745773 0.01745773 0.01745773 0.01745773 0.01745773 0.01745773\n",
            "  0.01745773 0.01745773 0.01745773 0.01745773]\n",
            " [0.01361346 0.01361346 0.01361346 0.01361346 0.01361346 0.01361346\n",
            "  0.01361346 0.01361346 0.01361346 0.01361346]]\n",
            "W2 = [[0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}