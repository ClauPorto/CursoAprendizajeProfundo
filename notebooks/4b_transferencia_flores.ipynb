{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04b_transferencia_flores.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/master/notebooks/04b_transferencia_flores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsEaPu6pFF_d",
        "colab_type": "text"
      },
      "source": [
        "# Aprendizaje por transferencia para el reconocimiento de flores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e2CgsEhowVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.layers import Add, ZeroPadding2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-1339ovz37o",
        "colab_type": "text"
      },
      "source": [
        "## Conjunto de datos\n",
        "Vamos a hacer aprendizaje por transferencia la red neuronal convolucional [MobileNetv2](https://arxiv.org/abs/1801.04381) para el reconocimiento de especies de flores usando el conjunto de imágenes [Flowers 102](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/), el cual está compuesto por imágenes a color de 102 especies diferentes de flores. \n",
        "\n",
        "_Tensorflow Datasets_ tiene funciones para descargar este conjunto de imágenes: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ere1HB9TjUV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = tfds.load(name='oxford_flowers102:2.*.*', with_info=False, as_supervised=True, split='train[:90%]')\n",
        "valid = tfds.load(name='oxford_flowers102:2.*.*', with_info=False, as_supervised=True, split='train[-10%:]')\n",
        "test = tfds.load(name='oxford_flowers102:2.*.*', with_info=False, as_supervised=True, split='test')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KjaFRwPF7j0",
        "colab_type": "text"
      },
      "source": [
        "## Acrecentamiento de datos\n",
        "Ahora vamos a definir nuestra tubería de datos para el entrenamiento, la cual tiene una función de mapeo que aplica ciertas transformaciones a las imágenes para incrementar su variedad. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjLJkFGVW7yA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def procesa_imagen(imagen, categoria, imsize=(160,160)):\n",
        "    imagen = tf.cast(imagen, 'float64')\n",
        "    imagen = tf.image.resize(imagen, imsize)\n",
        "    imagen /= 255.0\n",
        "    imagen = tf.image.random_flip_left_right(imagen)\n",
        "    imagen = tf.image.random_flip_up_down(imagen)\n",
        "    imagen = tf.image.random_crop(imagen, size=[imsize[0], imsize[1], 3])\n",
        "    imagen = tf.image.rot90(imagen, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
        "    imagen = tf.clip_by_value(imagen, 0, 1)\n",
        "    \n",
        "    return imagen, categoria\n",
        "\n",
        "ent_ds = train.map(procesa_imagen, num_parallel_calls=8)\n",
        "ent_ds = ent_ds.batch(64)\n",
        "ent_ds = ent_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "for i, (x, y_true) in enumerate(ent_ds, start=1):\n",
        "    plt.subplot(2, 5, i)\n",
        "    plt.imshow(x[0])\n",
        "    plt.axis('off')\n",
        "    if i == 10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8xMllJgGk5M",
        "colab_type": "text"
      },
      "source": [
        "Para el conjunto de validación y de prueba la tubería de datos no hace acrecentamiento de datos, solo redimensiona y normaliza las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac9WZKBKlEBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def carga_una_imagen(imagen, categoria):\n",
        "    imagen = tf.cast(imagen, 'float64')\n",
        "    imagen = tf.image.resize(imagen,(160,160))\n",
        "    imagen /= 255.0\n",
        "    return imagen, categoria\n",
        "  \n",
        "val_ds = valid.map(carga_una_imagen, num_parallel_calls=4)\n",
        "val_ds = val_ds.batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9JGfKYwG8cR",
        "colab_type": "text"
      },
      "source": [
        "## Definición de la red neuronal convolucional\n",
        "Ahora vamos a definir nuestra red neuronal convolucional. Tomaremos de base MobileNetv2, la cual es una red neuronal convolucional que ocupa convoluciones separables en profundidad. En particular, usaremos las capas convolucionales preentrenadas en el conjunto de imágenes de [ImageNet](http://www.image-net.org/), a las cuales le agregaremos capas de submuestreo y capas densas como etapa de clasificación. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhjFJjmpYDkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.base = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),\n",
        "                                                  include_top=False,\n",
        "                                                  weights='imagenet')\n",
        "    self.gap = GlobalAveragePooling2D()\n",
        "    self.dp = Dropout(0.7)\n",
        "    self.sm = Dense(102, \n",
        "                    activation='softmax', \n",
        "                    kernel_regularizer='l2', \n",
        "                    bias_regularizer='l2')\n",
        "\n",
        "  def call(self, x): \n",
        "    x = self.base(x)\n",
        "    x = self.gap(x)\n",
        "    x = self.dp(x)\n",
        "    x = self.sm(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZUv0vrvId83",
        "colab_type": "text"
      },
      "source": [
        "Construyamos nuestra red y visualicemos sus características:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzgH0TcsYLbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelo = CNN()\n",
        "modelo.build(input_shape=(None, 160, 160 , 3))\n",
        "modelo.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7y9ZRAU84XW",
        "colab_type": "text"
      },
      "source": [
        "Ahora definimos las funciones para realizar la propagación hacia adelante y la retropropagación en los ejemplos de entrenamiento y la propagación hacia adelante para los ejemplos de validación y de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LgAl9z-1Rdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def paso_ent(x, y, modelo, fn_perdida, optimizador):\n",
        "    with tf.GradientTape() as cinta:\n",
        "        y_pred = modelo(x)\n",
        "        perdida = fn_perdida(y, y_pred)\n",
        "    gradientes = cinta.gradient(perdida, modelo.trainable_variables)\n",
        "    optimizador.apply_gradients(zip(gradientes, modelo.trainable_variables))\n",
        "    return y_pred\n",
        "  \n",
        "@tf.function\n",
        "def paso_prueba(imagen, modelo):\n",
        "  return modelo(imagen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bDDPKJFmCYSF"
      },
      "source": [
        "## Entrenamiento preliminar\n",
        "Ya podemos entrenar nuestra red. Primero solo ajustaremos los pesos y sesgos de la etapa de clasificación, por lo que congelaremos los de las capas convolucionales. Como función de pérdida usaremos la función de entropía cruzada categórica:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3R1eNIoeVWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelo.base.trainable = False\n",
        "fn_perdida = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizador = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "perdida_ent = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "exactitud_ent = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "perdida_val = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "exactitud_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "hist_perdida_ent = []\n",
        "hist_exactitud_ent = []\n",
        "\n",
        "hist_perdida_val = []\n",
        "hist_exactitud_val = []\n",
        "\n",
        "for epoca in range(20):\n",
        "    for paso, (x, y) in enumerate(ent_ds):\n",
        "        y_pred = paso_ent(x, y,  modelo, fn_perdida, optimizador)\n",
        "        perdida_ent(y, y_pred)\n",
        "        exactitud_ent(y, y_pred)\n",
        "        \n",
        "    for (x, y) in val_ds:\n",
        "        y_pred = paso_prueba(x, modelo)\n",
        "        perdida_val(y, y_pred)\n",
        "        exactitud_val(y, y_pred)\n",
        "    \n",
        "    perdida_ent_res = perdida_ent.result().numpy()\n",
        "    exactitud_ent_res = exactitud_ent.result().numpy() * 100\n",
        "    perdida_ent.reset_states()\n",
        "    exactitud_ent.reset_states()\n",
        "    hist_perdida_ent.append(perdida_ent_res)\n",
        "    hist_exactitud_ent.append(exactitud_ent_res)\n",
        "\n",
        "    perdida_val_res = perdida_val.result().numpy()\n",
        "    exactitud_val_res = exactitud_val.result().numpy() * 100\n",
        "    perdida_val.reset_states()\n",
        "    exactitud_val.reset_states()\n",
        "    hist_perdida_val.append(perdida_val_res)\n",
        "    hist_exactitud_val.append(exactitud_val_res)\n",
        "  \n",
        "    print('E{:2d} CCE(E)={:6.2f}, Exactidud(E)={:6.2f} CCE(V)={:6.2f} Exactitud(V)={:6.2f}'.format(epoca, \n",
        "                                                                                                   perdida_ent_res, \n",
        "                                                                                                   exactitud_ent_res,\n",
        "                                                                                                   perdida_val_res, \n",
        "                                                                                                   exactitud_val_res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzG10IqA-sO-",
        "colab_type": "text"
      },
      "source": [
        "## Ajuste fino\n",
        "Después de ajustar por 5 épocas nuestras capas de clasificación, descongelaremos algunas de las últimas capas convolucionales y reanudaremos el entrenamiento por 5 épocas más. A este proceso se le conoce como ajuste fino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9-KRMve-woG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelo.base.trainable = True # descongela pesos y sesgos de todas las capas\n",
        "for layer in modelo.base.layers[:130]: # congela de nuevo primeras 130 capas\n",
        "  layer.trainable =  False\n",
        "\n",
        "for epoca in range(20,40):\n",
        "    for paso, (x, y) in enumerate(ent_ds):\n",
        "        y_pred = paso_ent(x, y,  modelo, fn_perdida, optimizador)\n",
        "        perdida_ent(y, y_pred)\n",
        "        exactitud_ent(y, y_pred)\n",
        "        \n",
        "    for (x, y) in val_ds:\n",
        "        y_pred = paso_prueba(x, modelo)\n",
        "        perdida_val(y, y_pred)\n",
        "        exactitud_val(y, y_pred)\n",
        "    \n",
        "    perdida_ent_res = perdida_ent.result().numpy()\n",
        "    exactitud_ent_res = exactitud_ent.result().numpy() * 100\n",
        "    perdida_ent.reset_states()\n",
        "    exactitud_ent.reset_states()\n",
        "    hist_perdida_ent.append(perdida_ent_res)\n",
        "    hist_exactitud_ent.append(exactitud_ent_res)\n",
        "\n",
        "    perdida_val_res = perdida_val.result().numpy()\n",
        "    exactitud_val_res = exactitud_val.result().numpy() * 100\n",
        "    perdida_val.reset_states()\n",
        "    exactitud_val.reset_states()\n",
        "    hist_perdida_val.append(perdida_val_res)\n",
        "    hist_exactitud_val.append(exactitud_val_res)\n",
        "  \n",
        "    print('E{:2d} CCE(E)={:6.2f}, Exactidud(E)={:6.2f} CCE(V)={:6.2f} Exactitud(V)={:6.2f}'.format(epoca, \n",
        "                                                                                                   perdida_ent_res, \n",
        "                                                                                                   exactitud_ent_res,\n",
        "                                                                                                   perdida_val_res, \n",
        "                                                                                                   exactitud_val_res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wWi4-tsKguy",
        "colab_type": "text"
      },
      "source": [
        "## Gráficas de la pérdida y la exactitud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rB05PJfW55Bh"
      },
      "source": [
        "Visualicemos la evolución de la pérdida de entropía cruzada y la exactitud durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23WTILcfSGlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot(hist_perdida_ent, color='red', label='Entrenamiento')\n",
        "plt.plot(hist_perdida_val, color='blue', label=u'Validación')\n",
        "plt.xlabel(u'Época')\n",
        "plt.ylabel(u'Pérdida')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(hist_exactitud_ent, color='red', label='Entrenamiento')\n",
        "plt.plot(hist_exactitud_val, color='blue', label=u'Validación')\n",
        "plt.xlabel(u'Época')\n",
        "plt.ylabel(u'Exactitud')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX02ClXTKn5k",
        "colab_type": "text"
      },
      "source": [
        "## Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuEsNw7DoKey",
        "colab_type": "text"
      },
      "source": [
        "Ya entrenado nuestro modelo, procedemos a evaluar su desempeño en el conjunto de imágenes de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHgbTjAfUvd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prueba_ds = test.map(carga_una_imagen, num_parallel_calls=4)\n",
        "prueba_ds = prueba_ds.batch(1)\n",
        "\n",
        "exactitud_prueba = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "for i, (x, y) in enumerate(prueba_ds):\n",
        "  y_pred = paso_prueba(x, modelo)\n",
        "  exactitud_prueba(y, y_pred)\n",
        "\n",
        "exactitud_prueba = exactitud_prueba.result().numpy() * 100\n",
        "print(\"Exactitud en validación: {:02.2f}%\".format(exactitud_prueba))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhqhww4NW8go",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, visualizamos algunos ejemplos de predicciónes hechas por el modelo en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RuXsa3VGlwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10, 4))\n",
        "for i, (x, y) in enumerate(prueba_ds, start=1):\n",
        "    ax = fig.add_subplot(2, 5, i)\n",
        "    ax.imshow(x[0])\n",
        "    y_pred = np.argmax(modelo(x))\n",
        "    ax.imshow(x[0])\n",
        "    text = 'V:' + str(y.numpy()[0]) + '  P:' + str(y_pred) \n",
        "    ax.set_title(text)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    if i == 10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}